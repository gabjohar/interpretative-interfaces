# Notes — Feb 18, 2026

## Environment
- Activate venv: `source venv/bin/activate` — exit with `deactivate`
- GPT-2 weights cached locally after first download (~500MB)

## Core concepts
- `model.run_with_cache(text)` —> runs GPT-2 and saves all intermediate activations
- `cache["resid_post", layer]` —> residual stream after a given layer, shape `(batch, tokens, 768)` -> investigate further
- Tokens include leading space (` cat` ≠ `cat`)

## Token trajectory insight (important for future viz)
`get_token_trajectory(text, token_index)` returns a token's 768-dim embedding at each of 12 layers.

Intuition:
- Early layers → "what word is this?"
- Middle layers → "what is near it?"
- Late layers → "what does it mean here?"

In Task 1.5, these trajectories (768 dims × 12 layers) are compressed to 2D via PCA, producing a viz showing the model's evolving interpretation of each word (but I don't really understand it)
