{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c08accc8-6cea-4a01-b7ba-86fb5c0da3e8",
   "metadata": {},
   "source": [
    "# Task 1.3: Tokenization Function + Notebook"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "59c62d9f-7a50-44fa-af2f-f0bc4e79f9c4",
   "metadata": {},
   "source": [
    "This notebook implements a `tokenize()` function and tests it with 3 different inputs."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d63fd143-3393-4f6c-8ace-450a426fc935",
   "metadata": {},
   "source": [
    "## Load the selected model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "856ade68-db7a-4301-bd72-eca07093a258",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "`torch_dtype` is deprecated! Use `dtype` instead!\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded pretrained model gpt2-small into HookedTransformer\n"
     ]
    }
   ],
   "source": [
    "from transformer_lens import HookedTransformer\n",
    "\n",
    "model = HookedTransformer.from_pretrained(\"gpt2-small\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7ba90b35-4b04-4b3e-a229-31fe63ef35b6",
   "metadata": {},
   "source": [
    "## The Actual Function\n",
    "\n",
    "This function uses TransformerLens to tokenize input text with GPT-2 small. It returns a list of dictionaries, including:\n",
    "- index: the token position\n",
    "- token_str: the token string (not guaranteed to be a full word)\n",
    "- token_id: the integer token ID used by the model\n",
    "\n",
    "Note that `model.to_tokens(text)` returns a tensor of shape `(batch, n_tokens)`. We then extract each token ID from the tensor and convert it to an integer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "62b7d606-cfda-486e-bd19-2f607b6f4e3b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenize(text: str) -> list[dict]:\n",
    "    \n",
    "    \"\"\"\n",
    "    Returns list of {index, token_str, token_id} for each token.\n",
    "    \"\"\"\n",
    "    \n",
    "    tokens = model.to_tokens(text) # tensor of token IDs\n",
    "    str_tokens = model.to_str_tokens(text) # list of strings\n",
    "    \n",
    "    return [\n",
    "        {\"index\": i, \"token_str\": s, \"token_id\": int(tokens[0, i])}\n",
    "        for i, s in enumerate(str_tokens)\n",
    "    ]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6339520a-9353-4971-a511-f624c4c94eba",
   "metadata": {},
   "source": [
    "## Test 1: Short Sentence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "99d0085c-174a-4e0f-b991-2856420c2a21",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'index': 0, 'token_str': '<|endoftext|>', 'token_id': 50256}\n",
      "{'index': 1, 'token_str': 'I', 'token_id': 40}\n",
      "{'index': 2, 'token_str': ' can', 'token_id': 460}\n",
      "{'index': 3, 'token_str': ' do', 'token_id': 466}\n",
      "{'index': 4, 'token_str': ' it', 'token_id': 340}\n",
      "{'index': 5, 'token_str': '!', 'token_id': 0}\n"
     ]
    }
   ],
   "source": [
    "test_result1 = tokenize(\"I can do it!\")\n",
    "## test_result1 = tokenize(\"Transformer is fun!\")\n",
    "\n",
    "for token in test_result1:\n",
    "    print(token)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0fb24492-54b5-4584-99fc-486de76a5531",
   "metadata": {},
   "source": [
    "## Test 2: Longer Sentence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "f76c1725-0b3e-4e4f-b6a2-0e28c949213e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'index': 0, 'token_str': '<|endoftext|>', 'token_id': 50256}\n",
      "{'index': 1, 'token_str': 'Gab', 'token_id': 46079}\n",
      "{'index': 2, 'token_str': 'riel', 'token_id': 11719}\n",
      "{'index': 3, 'token_str': 'le', 'token_id': 293}\n",
      "{'index': 4, 'token_str': ',', 'token_id': 11}\n",
      "{'index': 5, 'token_str': ' Mario', 'token_id': 10682}\n",
      "{'index': 6, 'token_str': ',', 'token_id': 11}\n",
      "{'index': 7, 'token_str': ' Polly', 'token_id': 36898}\n",
      "{'index': 8, 'token_str': ',', 'token_id': 11}\n",
      "{'index': 9, 'token_str': ' and', 'token_id': 290}\n",
      "{'index': 10, 'token_str': ' Steven', 'token_id': 8239}\n",
      "{'index': 11, 'token_str': ' are', 'token_id': 389}\n",
      "{'index': 12, 'token_str': ' building', 'token_id': 2615}\n",
      "{'index': 13, 'token_str': ' 3', 'token_id': 513}\n",
      "{'index': 14, 'token_str': ' AI', 'token_id': 9552}\n",
      "{'index': 15, 'token_str': ' interpret', 'token_id': 6179}\n",
      "{'index': 16, 'token_str': 'ability', 'token_id': 1799}\n",
      "{'index': 17, 'token_str': ' interfaces', 'token_id': 20314}\n",
      "{'index': 18, 'token_str': '.', 'token_id': 13}\n"
     ]
    }
   ],
   "source": [
    "test_result2 = tokenize(\"Gabrielle, Mario, Polly, and Steven are building 3 AI interpretability interfaces.\")\n",
    "\n",
    "for token in test_result2:\n",
    "    print(token)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "00604e04-cea1-40f3-b3b5-e58f239526d3",
   "metadata": {},
   "source": [
    "## Test 3: Sentence with Unusual Words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "99da4083-6940-4468-929d-39ae2c50d931",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'index': 0, 'token_str': '<|endoftext|>', 'token_id': 50256}\n",
      "{'index': 1, 'token_str': 'P', 'token_id': 47}\n",
      "{'index': 2, 'token_str': 'neum', 'token_id': 25668}\n",
      "{'index': 3, 'token_str': 'on', 'token_id': 261}\n",
      "{'index': 4, 'token_str': 'oult', 'token_id': 25955}\n",
      "{'index': 5, 'token_str': 'ram', 'token_id': 859}\n",
      "{'index': 6, 'token_str': 'icro', 'token_id': 2500}\n",
      "{'index': 7, 'token_str': 'sc', 'token_id': 1416}\n",
      "{'index': 8, 'token_str': 'op', 'token_id': 404}\n",
      "{'index': 9, 'token_str': 'ics', 'token_id': 873}\n",
      "{'index': 10, 'token_str': 'ilic', 'token_id': 41896}\n",
      "{'index': 11, 'token_str': 'ov', 'token_id': 709}\n",
      "{'index': 12, 'token_str': 'ol', 'token_id': 349}\n",
      "{'index': 13, 'token_str': 'can', 'token_id': 5171}\n",
      "{'index': 14, 'token_str': 'ocon', 'token_id': 36221}\n",
      "{'index': 15, 'token_str': 'iosis', 'token_id': 42960}\n",
      "{'index': 16, 'token_str': ' is', 'token_id': 318}\n",
      "{'index': 17, 'token_str': ' an', 'token_id': 281}\n",
      "{'index': 18, 'token_str': ' unbelievably', 'token_id': 48943}\n",
      "{'index': 19, 'token_str': ' long', 'token_id': 890}\n",
      "{'index': 20, 'token_str': ' word', 'token_id': 1573}\n",
      "{'index': 21, 'token_str': ' to', 'token_id': 284}\n",
      "{'index': 22, 'token_str': ' test', 'token_id': 1332}\n",
      "{'index': 23, 'token_str': ' with', 'token_id': 351}\n",
      "{'index': 24, 'token_str': ' G', 'token_id': 402}\n",
      "{'index': 25, 'token_str': 'PT', 'token_id': 11571}\n",
      "{'index': 26, 'token_str': '-', 'token_id': 12}\n",
      "{'index': 27, 'token_str': '2', 'token_id': 17}\n",
      "{'index': 28, 'token_str': ' small', 'token_id': 1402}\n",
      "{'index': 29, 'token_str': '!', 'token_id': 0}\n"
     ]
    }
   ],
   "source": [
    "test_result3 = tokenize(\"Pneumonoultramicroscopicsilicovolcanoconiosis is an unbelievably long word to test with GPT-2 small!\")\n",
    "\n",
    "for token in test_result3:\n",
    "    print(token)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5a16b40f-34c4-405c-aca1-27d3ea22a02f",
   "metadata": {},
   "source": [
    "## Observations\n",
    "\n",
    "- GPT-2 automatically prepends a `<|endoftext|>` token at index 0. This behavior can be disabled using `prepend_bos=False` when converting text to tokens or token strings, though keeping it helps maintain consistent indexing especially with later activations.\n",
    "- Tokens are not always full words. For instance, \"Transformer\" was split into \"Trans\" and \"former\", and a rare and very long word in Test 3 was split into many smaller pieces.\n",
    "- Spaces were included inside several tokens (e.g. `' is'` or `' word'`).\n",
    "- Punctuation and symbols, such as `'-'` and `'!'`, and numbers were often treated as separate tokens.\n",
    "- Same token (e.g. `' is'`) always maps to the same token ID, but note that `' is'` and `'is'` are treated as ***different*** tokens and thus have different token IDs (e.g. in my test, `' can'` has `'token_id': 460` while `'can'` has `'token_id': 5171`)."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
