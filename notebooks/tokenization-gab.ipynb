{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Task 1.3: Tokenization Function\n",
    "\n",
    "This notebook implements a `tokenize()` function using TransformerLens and tests it on 3 different inputs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "`torch_dtype` is deprecated! Use `dtype` instead!\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded pretrained model gpt2-small into HookedTransformer\n"
     ]
    }
   ],
   "source": [
    "from transformer_lens import HookedTransformer\n",
    "\n",
    "model = HookedTransformer.from_pretrained(\"gpt2-small\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenize(text: str) -> list[dict]:\n",
    "    \"\"\"Returns list of {index, token_str, token_id} for each token.\"\"\"\n",
    "    tokens = model.to_tokens(text)          # tensor of token IDs, shape (1, n_tokens)\n",
    "    str_tokens = model.to_str_tokens(text)  # list of strings\n",
    "    return [\n",
    "        {\"index\": i, \"token_str\": s, \"token_id\": int(tokens[0, i])}\n",
    "        for i, s in enumerate(str_tokens)\n",
    "    ]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test 1: Short sentence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'index': 0, 'token_str': '<|endoftext|>', 'token_id': 50256}\n",
      "{'index': 1, 'token_str': 'Hello', 'token_id': 15496}\n",
      "{'index': 2, 'token_str': ' world', 'token_id': 995}\n"
     ]
    }
   ],
   "source": [
    "result1 = tokenize(\"Hello world\")\n",
    "for token in result1:\n",
    "    print(token)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test 2: Longer sentence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'index': 0, 'token_str': '<|endoftext|>', 'token_id': 50256}\n",
      "{'index': 1, 'token_str': 'The', 'token_id': 464}\n",
      "{'index': 2, 'token_str': ' quick', 'token_id': 2068}\n",
      "{'index': 3, 'token_str': ' brown', 'token_id': 7586}\n",
      "{'index': 4, 'token_str': ' fox', 'token_id': 21831}\n",
      "{'index': 5, 'token_str': ' jumps', 'token_id': 18045}\n",
      "{'index': 6, 'token_str': ' over', 'token_id': 625}\n",
      "{'index': 7, 'token_str': ' the', 'token_id': 262}\n",
      "{'index': 8, 'token_str': ' lazy', 'token_id': 16931}\n",
      "{'index': 9, 'token_str': ' dog', 'token_id': 3290}\n",
      "{'index': 10, 'token_str': ' near', 'token_id': 1474}\n",
      "{'index': 11, 'token_str': ' the', 'token_id': 262}\n",
      "{'index': 12, 'token_str': ' tower', 'token_id': 10580}\n",
      "{'index': 13, 'token_str': '.', 'token_id': 13}\n"
     ]
    }
   ],
   "source": [
    "result2 = tokenize(\"The quick brown fox jumps over the lazy dog near the tower.\")\n",
    "for token in result2:\n",
    "    print(token)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test 3: Sentence with unusual words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'index': 0, 'token_str': '<|endoftext|>', 'token_id': 50256}\n",
      "{'index': 1, 'token_str': 'The', 'token_id': 464}\n",
      "{'index': 2, 'token_str': ' transformer', 'token_id': 47385}\n",
      "{'index': 3, 'token_str': \"'s\", 'token_id': 338}\n",
      "{'index': 4, 'token_str': ' hippocampus', 'token_id': 38587}\n",
      "{'index': 5, 'token_str': '-', 'token_id': 12}\n",
      "{'index': 6, 'token_str': 'like', 'token_id': 2339}\n",
      "{'index': 7, 'token_str': ' architecture', 'token_id': 10959}\n",
      "{'index': 8, 'token_str': ' is', 'token_id': 318}\n",
      "{'index': 9, 'token_str': ' unparalleled', 'token_id': 39235}\n",
      "{'index': 10, 'token_str': 'ly', 'token_id': 306}\n",
      "{'index': 11, 'token_str': ' fascinating', 'token_id': 13899}\n",
      "{'index': 12, 'token_str': '!', 'token_id': 0}\n"
     ]
    }
   ],
   "source": [
    "result3 = tokenize(\"The transformer's hippocampus-like architecture is unparalleledly fascinating!\")\n",
    "for token in result3:\n",
    "    print(token)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Observations\n",
    "\n",
    "Things to note as you run the cells above:\n",
    "\n",
    "- **BOS token**: GPT-2 prepends a special `<|endoftext|>` token (index 0) to every input\n",
    "- **Subword tokenization**: long or unusual words get split into pieces (e.g. `unparalleled` → `un` + `parallel` + `eled`)\n",
    "- **Spaces are part of tokens**: notice tokens often start with a space (shown as `Ġ` internally), so `\" world\"` is one token, not `\"world\"`\n",
    "- **Punctuation**: punctuation is usually its own token"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
